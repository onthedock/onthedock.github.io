<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on On The Dock</title>
    <link>https://onthedock.github.io/tags/kubernetes/</link>
    <description>Recent content in Kubernetes on On The Dock</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>Handmade with &amp;#9829; by Xavi Aznar</copyright>
    <lastBuildDate>Sun, 26 Mar 2023 17:38:10 +0200</lastBuildDate>
    <atom:link href="https://onthedock.github.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>k3d: Registry local en Kubernetes</title>
      <link>https://onthedock.github.io/post/230326-k3d-registry-local-en-k8s/</link>
      <pubDate>Sun, 26 Mar 2023 17:38:10 +0200</pubDate>
      <guid>https://onthedock.github.io/post/230326-k3d-registry-local-en-k8s/</guid>
      <description>&lt;p&gt;En la entrada anterior &lt;a href=&#34;https://onthedock.github.io/post/230326-k3d-cluster-como-codigo/&#34;&gt;k3d: desplegar un clúster de Kubernetes como código&lt;/a&gt; incluí un &lt;em&gt;registry&lt;/em&gt; en el despliegue del clúster con &lt;strong&gt;k3d&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;En esta entrada veremos cómo usarlo para desplegar aplicaciones en el clúster.&lt;/p&gt;</description>
    </item>
    <item>
      <title>k3d: desplegar un clúster de Kubernetes como código</title>
      <link>https://onthedock.github.io/post/230326-k3d-cluster-como-codigo/</link>
      <pubDate>Sun, 26 Mar 2023 12:20:00 +0200</pubDate>
      <guid>https://onthedock.github.io/post/230326-k3d-cluster-como-codigo/</guid>
      <description>&lt;p&gt;Ya he hablado de &lt;a href=&#34;https://k3d.io/&#34;&gt;k3d&lt;/a&gt; otras veces en el blog; del mismo modo que &lt;a href=&#34;https://kind.sigs.k8s.io/&#34;&gt;kind&lt;/a&gt; permite desplegar Kubernetes en Docker: cada &lt;em&gt;nodo&lt;/em&gt; del clúster se ejecuta en un contenedor.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;k3d&lt;/strong&gt; hace lo mismo pero en vez de Kubernetes &lt;em&gt;vanilla&lt;/em&gt;, usa la distribución ligera de Kubernetes, &lt;a href=&#34;https://k3s.io/&#34;&gt;k3s&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Este fin de semana he actualizado la documentación del repositorio &lt;a href=&#34;https://github.com/onthedock/k8s-devops&#34;&gt;onthedock/k8s-devops&lt;/a&gt; en lo relativo a k3d y he aprovechado para explorar un par de cosas nuevas: deplegar el clúster de forma declarativa (como código) y el uso del &lt;em&gt;registry&lt;/em&gt; interno en k3s.&lt;/p&gt;&#xA;&lt;p&gt;En este artículo me centro en la primera parte: el despliegue del clúster como código.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Clúster de K3s: actualización del script de despliegue</title>
      <link>https://onthedock.github.io/post/230108-actualizando-el-script-de-despliegue-del-cluster-de-k3s/</link>
      <pubDate>Sun, 08 Jan 2023 13:35:14 +0100</pubDate>
      <guid>https://onthedock.github.io/post/230108-actualizando-el-script-de-despliegue-del-cluster-de-k3s/</guid>
      <description>&lt;p&gt;Después de un tiempo focalizado casi exclusivamente en aprender Go, hoy he vuelto a &lt;em&gt;mis raíces&lt;/em&gt;: Kubernetes.&lt;/p&gt;&#xA;&lt;p&gt;En el repositorio &lt;a href=&#34;https://github.com/onthedock/vagrant&#34;&gt;onthedock/vagrant&lt;/a&gt; tengo los &lt;em&gt;scripts&lt;/em&gt; que me permiten desplegar varias máquinas virtuales usando Vagrant, instalar K3s y configurar un clúster de Kubernetes con (en este momento) un nodo &lt;em&gt;master&lt;/em&gt; y dos &lt;em&gt;workers&lt;/em&gt;. Como parte de la automatización, también despliego Longhorn como &lt;em&gt;storageClass&lt;/em&gt; .&lt;/p&gt;&#xA;&lt;p&gt;Hoy he testeado con éxito el despliegue de ArgoCD y Gitea en el clúster, dando un pasito adelante para desplegar una plataforma completa de desarrollo sobre Kubernetes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Error en k3sup: Unable to Connect to Server over SSH (Ubuntu 22.04)</title>
      <link>https://onthedock.github.io/post/220509-error-en-k3sup-unable-to-connect-to-server-ubuntu-22-04/</link>
      <pubDate>Mon, 09 May 2022 21:43:30 +0200</pubDate>
      <guid>https://onthedock.github.io/post/220509-error-en-k3sup-unable-to-connect-to-server-ubuntu-22-04/</guid>
      <description>&lt;p&gt;Tras solucionar los problemas con Vagrant al actualizar a PoP_OS! 22.04 (basada en Ubuntu 22.04), encuentro otro problema relacionado también con SSH :(&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/alexellis/k3sup&#34;&gt;k3sup&lt;/a&gt;, el instalador de clústers de Kubernetes usando &lt;a href=&#34;https://k3s.io/&#34;&gt;k3s&lt;/a&gt;, no puede conectar con las máquinas virtuales basadas en la imagen &lt;a href=&#34;https://app.vagrantup.com/ubuntu/boxes/jammy64&#34;&gt;ubuntu/jammy64&lt;/a&gt; (la nueva versión de Ubuntu 22.04).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Obtener el ID de un contenedor #discuss.kubernetes.io</title>
      <link>https://onthedock.github.io/post/220321-obtener-el-id-de-un-contenedor/</link>
      <pubDate>Mon, 21 Mar 2022 22:14:17 +0100</pubDate>
      <guid>https://onthedock.github.io/post/220321-obtener-el-id-de-un-contenedor/</guid>
      <description>&lt;p&gt;Estaba leyendo las nuevas entradas en el foro de Kubernetes y me he encontrado con &lt;a href=&#34;https://discuss.kubernetes.io/t/getting-pod-id-and-container-id-of-a-container-when-it-restarts/19413&#34;&gt;Getting Pod ID and container ID of a container when it restarts&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;La pregunta es cómo obtener el identificador de un &lt;em&gt;pod&lt;/em&gt; (y de un contenedor dentro del &lt;em&gt;pod&lt;/em&gt;) cuando éste se reinicia. Lo curioso -al menos para mí- es que esa información es necesaria porque hay &lt;em&gt;otro pod&lt;/em&gt; que monitoriza el primero que necesita esta información (supondo que para identificar el &lt;em&gt;pod&lt;/em&gt; monitorizado).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Actualizar ArgoCD - ¡ojo! se sobrescriben los ConfigMap de configuración</title>
      <link>https://onthedock.github.io/post/220213-actualizar-argocd/</link>
      <pubDate>Sun, 13 Feb 2022 09:04:04 +0100</pubDate>
      <guid>https://onthedock.github.io/post/220213-actualizar-argocd/</guid>
      <description>&lt;p&gt;A raíz de la entrada anterior &lt;a href=&#34;https://onthedock.github.io/post/220212-instalacion-y-acceso-a-la-consola/&#34;&gt;GitOps con ArgoCD - Instalación y acceso a la consola&lt;/a&gt; he visto que la versión desplegada en el clúster de laboratorio era la 2.2.1, mientras que la versión actual es la 2.2.5.&lt;/p&gt;&#xA;&lt;p&gt;Antes de actualizar, al tratarse de una versión &lt;em&gt;patch release&lt;/em&gt;, no es necesario tener en cuenta ninguna consideración especial, según se indica en la documentación oficial &lt;a href=&#34;https://argo-cd.readthedocs.io/en/stable/operator-manual/upgrading/overview/&#34;&gt;Upgrading &amp;gt; Overview&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Sin embargo, aplicando el fichero correspondiente a la última versión estable, &lt;strong&gt;se sobreescriben los &lt;em&gt;ConfigMaps&lt;/em&gt; de configuración&lt;/strong&gt;, por lo que las modificaciones realizadas se pierden; en mi caso, la configuración del modo &lt;em&gt;inseguro&lt;/em&gt; necesario para el acceso a través de un &lt;em&gt;ingress&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Como solución temporal, se puede aplicar de nuevo el fichero con el &lt;em&gt;ConfigMap&lt;/em&gt; y ejecutar &lt;code&gt;kubectl -n argocd rollout restart deploy argocd-server&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>GitOps con ArgoCD - Instalación y acceso a la consola</title>
      <link>https://onthedock.github.io/post/220212-instalacion-y-acceso-a-la-consola/</link>
      <pubDate>Sat, 12 Feb 2022 15:39:46 +0100</pubDate>
      <guid>https://onthedock.github.io/post/220212-instalacion-y-acceso-a-la-consola/</guid>
      <description>&lt;p&gt;GitOps es una forma de gestionar los clústers de Kubernetes y el proceso de &lt;em&gt;application delivery&lt;/em&gt;, según consta en la definición que hacen los inventores del término, el equipo de Weave.works en &lt;a href=&#34;https://www.weave.works/technologies/gitops/&#34;&gt;What is GitOps?&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;El concepto &lt;em&gt;gitOps&lt;/em&gt; proporciona un modelo operativo en el que &lt;em&gt;el estado deseado&lt;/em&gt; del clúster (y de las aplicaciones desplegadas en él) se encuentra definido de forma &lt;strong&gt;declarativa&lt;/strong&gt; en un repositorio Git.&lt;/p&gt;&#xA;&lt;p&gt;Un agente se encarga de reconciliar el &lt;em&gt;estado deseado&lt;/em&gt; (en Git) con el &lt;em&gt;estado real&lt;/em&gt; (en Kubernetes), considerando -en general- como &lt;strong&gt;fuente de la verdad&lt;/strong&gt; el contenido del repositorio.&lt;/p&gt;&#xA;&lt;p&gt;Aunque Weave.works desarrolló inicialmente &lt;a href=&#34;https://fluxcd.io/&#34;&gt;Flux&lt;/a&gt; (ahora forma parte de la CNCF), en este &lt;em&gt;post&lt;/em&gt; hablaré de &lt;a href=&#34;https://argo-cd.readthedocs.io/en/stable/&#34;&gt;ArgoCD&lt;/a&gt;. Hay otras herramientas con las que implementar GitOps, pero sin duda Flux y ArgoCD son las referencias indiscutibles.&lt;/p&gt;</description>
    </item>
    <item>
      <title>¿Por qué los Deployments usan Replicasets pero los Statefulsets y los Daemonsets no?</title>
      <link>https://onthedock.github.io/post/211230-porque-los-deployments-usan-replicasets-pero-no-los-statefulsets-ni-los-daemonsets/</link>
      <pubDate>Thu, 30 Dec 2021 12:12:02 +0100</pubDate>
      <guid>https://onthedock.github.io/post/211230-porque-los-deployments-usan-replicasets-pero-no-los-statefulsets-ni-los-daemonsets/</guid>
      <description>&lt;p&gt;Revisando el &lt;em&gt;feed&lt;/em&gt; del foro de Kubernetes &lt;a href=&#34;https://discuss.kubernetes.io/&#34;&gt;discuss.kubernetes.io&lt;/a&gt;, me llamó la atención la pregunta de &lt;code&gt;user2&lt;/code&gt; &lt;a href=&#34;https://discuss.kubernetes.io/t/why-deployment-need-replicaset-but-daemonset-and-statefulset-dont-need/18334&#34;&gt;Why deployment need replicaset, but daemonset and statefulset don’t need&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://discuss.kubernetes.io/t/why-deployment-need-replicaset-but-daemonset-and-statefulset-dont-need/18334/2?u=xavi&#34;&gt;Respondí en el foro&lt;/a&gt;, pero quiero ampliar la respuesta aquí.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Crear usuarios (usando recursos nativos) en Kubernetes 1.19&#43;</title>
      <link>https://onthedock.github.io/post/211205-crear-usuarios-usando-recursos-nativos-en-kubernetes-1.19/</link>
      <pubDate>Sun, 05 Dec 2021 19:57:28 +0100</pubDate>
      <guid>https://onthedock.github.io/post/211205-crear-usuarios-usando-recursos-nativos-en-kubernetes-1.19/</guid>
      <description>&lt;p&gt;Hace unas entradas, en &lt;a href=&#34;https://onthedock.github.io/post/211010-crear-usuarios-en-k3s/&#34;&gt;&#xA;Crear usuarios en Kubernetes (y en K3s)&lt;/a&gt;, escribía sobre cómo generar nuevos usuarios con acceso al clúster de Kubernetes usando un fichero &lt;code&gt;kubeconfig&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;El método descrito implicaba extraer fuera del clúster el certificado privado de la entidad certificadora (CA) de Kubernetes, lo que no me parecía la mejor solución.&lt;/p&gt;&#xA;&lt;p&gt;Desde Kubernetes 1.19 existe un nuevo recurso en la API, el &lt;code&gt;CertificateSigningRequest&lt;/code&gt;, que permite firmar certificados para proporcionar acceso (por ejemplo) al clúster.&lt;/p&gt;&#xA;&lt;p&gt;En esta entrada se describe cómo aprovechar esta nueva funcionalidad para dar acceso a un usuario usando un certificado firmado por la CA del clúster.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Automatiza la instalación de Longhorn</title>
      <link>https://onthedock.github.io/post/211113-automatiza-la-instalacion-de-longhorn/</link>
      <pubDate>Sat, 13 Nov 2021 19:53:33 +0100</pubDate>
      <guid>https://onthedock.github.io/post/211113-automatiza-la-instalacion-de-longhorn/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://longhorn.io/&#34;&gt;Longhorn&lt;/a&gt; es una solución de almacenamiento distribuido de bloques para Kubernetes. Recientemente ha sido incluido en la &lt;a href=&#34;https://www.cncf.io/blog/2021/11/04/longhorn-brings-cloud-native-distributed-storage-to-the-cncf-incubator/&#34;&gt;&lt;em&gt;incubadora&lt;/em&gt; de la CNCF&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Longhorn proporciona métricas para Prometheus y es el complemento perfecto para proporcionar almacenamiento a las aplicaciones desplegadas sobre Kubernetes.&lt;/p&gt;&#xA;&lt;p&gt;En esta entrada automatizamos las &lt;a href=&#34;https://longhorn.io/docs/1.2.2/deploy/install/install-with-helm/&#34;&gt;instrucciones oficiales&lt;/a&gt; de despliegue usando Helm para desplegarlo sobre Kubernetes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Actualizar k3s con k3sup</title>
      <link>https://onthedock.github.io/post/211111-actualizar-k3s-con-k3sup/</link>
      <pubDate>Thu, 11 Nov 2021 21:32:41 +0100</pubDate>
      <guid>https://onthedock.github.io/post/211111-actualizar-k3s-con-k3sup/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/alexellis/k3sup&#34;&gt;k3sup&lt;/a&gt; es una herramienta que permite instalar clústers de Kubernetes basados en &lt;a href=&#34;https://k3s.io/&#34;&gt;K3s&lt;/a&gt; &lt;a href=&#34;https://github.com/alexellis/k3sup#demo-&#34;&gt;en menos de un minuto&lt;/a&gt;. Pero acabo de descubrir que además, también permite actualizar el clúster.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Script para crear usuarios en Kubernetes (y en K3s)</title>
      <link>https://onthedock.github.io/post/211010-crear-usuarios-en-k3s-script/</link>
      <pubDate>Sun, 10 Oct 2021 08:51:51 +0200</pubDate>
      <guid>https://onthedock.github.io/post/211010-crear-usuarios-en-k3s-script/</guid>
      <description>&lt;p&gt;El &lt;em&gt;script&lt;/em&gt; automatiza el proceso completo de creación de un usuario en Kubernetes. Como el proceso es algo diferente en K3s, esta entrada se centra más en este caso &lt;em&gt;especial&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Crear usuarios en Kubernetes (y en K3s)</title>
      <link>https://onthedock.github.io/post/211010-crear-usuarios-en-k3s/</link>
      <pubDate>Sun, 10 Oct 2021 07:08:56 +0200</pubDate>
      <guid>https://onthedock.github.io/post/211010-crear-usuarios-en-k3s/</guid>
      <description>&lt;p&gt;En Kubernetes no existe el concepto de usuario; sólo se confía en quien presente un certificado firmado por la CA del clúster.&lt;/p&gt;&#xA;&lt;p&gt;Para obtener los certificados de la CA, lo más sencillo es acceder a un nodo &lt;em&gt;server&lt;/em&gt;; los certificados (&lt;code&gt;ca.cert&lt;/code&gt; y &lt;code&gt;ca.key&lt;/code&gt;) se encuentran en &lt;code&gt;/etc/kubernetes/pki/&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Existe otra opción que pasa por generar un objeto &lt;code&gt;CertificateSigningRequest&lt;/code&gt; para firmar el certificado de usuario.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;El proceso implica generar un certificado al usuario, solicitar que lo firme la &lt;em&gt;Certificate Authority&lt;/em&gt; del clúster y después autenticarse con él.&lt;/p&gt;&#xA;&lt;p&gt;Para poder autenticarse en el clúster, necesitamos configurar un cliente, por ejemplo creando un fichero &lt;code&gt;kubeconfig&lt;/code&gt; para &lt;strong&gt;kubectl&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Finalmente, el nuevo usuario debe estar autorizado a realizar algunas acciones en el clúster; para ello definiremos un conjunto de permisos en un &lt;em&gt;Role&lt;/em&gt; o un &lt;em&gt;ClusterRole&lt;/em&gt; y lo asociaremos al usuario mediante un &lt;em&gt;RoleBinding&lt;/em&gt; (o un &lt;em&gt;ClusterRoleBinding&lt;/em&gt;).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Provisionar Kubernetes con Vagrant y K3sup - 2a parte</title>
      <link>https://onthedock.github.io/post/210920-provisionar-kubernetes-con-vagrant-y-k3sup-2/</link>
      <pubDate>Mon, 20 Sep 2021 19:25:15 +0200</pubDate>
      <guid>https://onthedock.github.io/post/210920-provisionar-kubernetes-con-vagrant-y-k3sup-2/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://onthedock.github.io/post/210919-provisionar-kubernetes-con-vagrant-y-k3sup-1/&#34;&gt;Provisionar Kubernetes con Vagrant y K3sup - 1a parte&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://onthedock.github.io/post/210920-provisionar-kubernetes-con-vagrant-y-k3sup-2/&#34;&gt;Provisionar Kubernetes con Vagrant y K3sup - 2a parte&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;En la entrada anterior, tenemos un fichero &lt;code&gt;Vagrantfile&lt;/code&gt; que permite provisionar máquinas virtuales a partir de la imagen base seleccionada. Las máquinas generadas están configuradas a nivel de hipervisor, con los recursos de CPU y memoria elegidos. También se ha configurado el nombre de la máquina virtual y se ha establecido una IP estática. También se han deshabilitado algunos aspectos específicos de Vagrant, como las carpetas compartidas o la comprobación de actualizaciones de la imagen base.&lt;/p&gt;&#xA;&lt;p&gt;En esta segunda parte nos centramos en la configuración del sistema operativo aprovechando la capacidad de Vagrant de ejecutar &lt;em&gt;scripts&lt;/em&gt; en las máquinas creadas.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Provisionar Kubernetes con Vagrant y K3sup - 1a parte</title>
      <link>https://onthedock.github.io/post/210919-provisionar-kubernetes-con-vagrant-y-k3sup-1/</link>
      <pubDate>Sun, 19 Sep 2021 10:31:17 +0200</pubDate>
      <guid>https://onthedock.github.io/post/210919-provisionar-kubernetes-con-vagrant-y-k3sup-1/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://onthedock.github.io/post/210919-provisionar-kubernetes-con-vagrant-y-k3sup-1/&#34;&gt;Provisionar Kubernetes con Vagrant y K3sup - 1a parte&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://onthedock.github.io/post/210920-provisionar-kubernetes-con-vagrant-y-k3sup-2/&#34;&gt;Provisionar Kubernetes con Vagrant y K3sup - 2a parte&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Desde mis inicios con Kubernetes, una de las cosas más pesadas del proceso de creación del clúster (para mí) ha sido el tener que desplegar y configurar las máquinas que formarán el clúster.&lt;/p&gt;&#xA;&lt;p&gt;En esta entrada explico algo de mi relación histórica con Vagrant y el proceso de automatización que estoy siguiendo para desplegar clústers locales (&lt;em&gt;no cloud&lt;/em&gt;) en mi laboratorio.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Creación de Sealed Secrets (SealedSecrets-II)</title>
      <link>https://onthedock.github.io/post/210819-sealed-secrets-ii/</link>
      <pubDate>Thu, 19 Aug 2021 21:12:30 +0200</pubDate>
      <guid>https://onthedock.github.io/post/210819-sealed-secrets-ii/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://onthedock.github.io/post/210819-sealed-secrets/&#34;&gt;Instalación de SealedSecrets (SealedSecrets - I)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://onthedock.github.io/post/210819-sealed-secrets-ii/&#34;&gt;Creación de SealedSecrets (SealedSecrets - II)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Instalación de Sealed Secrets (SealedSecrets-I)</title>
      <link>https://onthedock.github.io/post/210819-sealed-secrets/</link>
      <pubDate>Thu, 19 Aug 2021 20:18:20 +0200</pubDate>
      <guid>https://onthedock.github.io/post/210819-sealed-secrets/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://onthedock.github.io/post/210819-sealed-secrets/&#34;&gt;Instalación de SealedSecrets (SealedSecrets - I)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://onthedock.github.io/post/210819-sealed-secrets-ii/&#34;&gt;Creación de SealedSecrets (SealedSecrets - II)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Uno de los problemas abiertos de Kubernetes es la gestión de los &lt;em&gt;secretos&lt;/em&gt;; es decir, aquella información sensible que, al menos de momento, se guarda en texto plano y cuya única medida de &amp;ldquo;seguridad&amp;rdquo; consiste en codificarla en &lt;a href=&#34;https://es.wikipedia.org/wiki/Base64&#34;&gt;base64&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Tal y como se indica en el &lt;code&gt;README.md&lt;/code&gt; de &lt;a href=&#34;https://github.com/bitnami-labs/sealed-secrets&#34;&gt;Sealed Secrets&lt;/a&gt;, &lt;strong&gt;puedes gestionar toda la configuración de Kubernetes en Git&amp;hellip; excepto los &lt;em&gt;Secrets&lt;/em&gt;&lt;/strong&gt;, precisamente porque los &lt;em&gt;Secrets&lt;/em&gt; no son seguros&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;Con el auge de metodologías como &lt;a href=&#34;https://www.weave.works/technologies/gitops/&#34;&gt;GitOps&lt;/a&gt;, el problema es mayor, ya que los desarrolladores no tienen acceso directo al clúster y &lt;strong&gt;todo&lt;/strong&gt; debe desplegarse automáticamente desde Git.&lt;/p&gt;&#xA;&lt;p&gt;Una de las soluciones al problema viene del equipo de &lt;a href=&#34;https://github.com/bitnami-labs/&#34;&gt;Bitnami-Labs&lt;/a&gt; con &lt;strong&gt;SealedSecrets&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Documentación como código - 2a parte</title>
      <link>https://onthedock.github.io/post/210816-documentacion-como-codigo-prueba-de-concepto-2a-parte/</link>
      <pubDate>Mon, 16 Aug 2021 20:16:34 +0200</pubDate>
      <guid>https://onthedock.github.io/post/210816-documentacion-como-codigo-prueba-de-concepto-2a-parte/</guid>
      <description>&lt;p&gt;En la &lt;a href=&#34;https://onthedock.github.io/post/210731-documentacion-como-codigo-poc-1a-parte/&#34;&gt;entrada anterior&lt;/a&gt; indicaba la idea general en la que estoy trabajando para implementar una solución funcional de &lt;em&gt;documentación como código&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Reducida a su mínima expresión, la prueba de concepto lo que tiene que mostrar es la &lt;em&gt;velocidad&lt;/em&gt; a la que se puede ir actualizando la documentación si se sigue el mismo proceso -y herramientas- de desarrollo a las que está acostumbrado el equipo de proyecto.&lt;/p&gt;&#xA;&lt;p&gt;No se trata de crear un sistema listo para producción, sino de mostrar &lt;em&gt;algo&lt;/em&gt; que &lt;strong&gt;funcione&lt;/strong&gt; ™ más o menos, como funcionaría la solución final.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Documentación como código - 1a parte</title>
      <link>https://onthedock.github.io/post/210731-documentacion-como-codigo-poc-1a-parte/</link>
      <pubDate>Sat, 31 Jul 2021 07:33:11 +0200</pubDate>
      <guid>https://onthedock.github.io/post/210731-documentacion-como-codigo-poc-1a-parte/</guid>
      <description>&lt;p&gt;Una de las claves del éxito de un proyecto es proporcionar una buena documentación. En los proyectos &lt;em&gt;open source&lt;/em&gt; cada vez se da más importancia a tener documentación de calidad y completamente actualizada.&lt;/p&gt;&#xA;&lt;p&gt;En el pasado -aunque tristemente, sigue siendo una práctica muy habitual- la documentación se dejaba como una tarea a realizar una vez el proyecto estuviera prácticamente completado, antes de la entrega al cliente. El problema de esta aproximación es que los proyectos suelen encontrar problemas que hace que las fechas previstas inicialmente no se cumplan, o que se cumplan &lt;em&gt;artificialmente&lt;/em&gt; entregando sin haber completado tareas como por ejemplo, la documentación.&lt;/p&gt;&#xA;&lt;p&gt;Con la introducción de las metodologías ágiles, en cada tarea que se crea en el backlog se incluye la documentación como parte del criterio de aceptación, explícita o implícitamente.&lt;/p&gt;&#xA;&lt;p&gt;En vez de generar un documento enorme, se suele optar por formatos ligeros como &lt;a href=&#34;https://daringfireball.net/projects/markdown/&#34;&gt;markdown&lt;/a&gt;, como paso previo a generar una versión web para el usuario final. Gracias a herramientas como &lt;a href=&#34;https://pandoc.org/&#34;&gt;pandoc&lt;/a&gt;, también es fácil generar documentación final en prácticamente cualquier otro formato que se requiera, como los habituales Microsoft Word o PDF.&lt;/p&gt;&#xA;&lt;p&gt;De esta forma, el equipo de desarrollo trabaja en paralelo en la creación de nuevas funcionalidades a la vez que las documenta.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Instalación de Gitea con Helm</title>
      <link>https://onthedock.github.io/post/210530-instalacion-de-gitea-con-helm/</link>
      <pubDate>Sun, 30 May 2021 09:28:14 +0200</pubDate>
      <guid>https://onthedock.github.io/post/210530-instalacion-de-gitea-con-helm/</guid>
      <description>&lt;p&gt;En la entrada anterior, &lt;a href=&#34;https://onthedock.github.io/post/210529-revision-de-la-chart-de-gitea/&#34;&gt;Revisión de la Helm chart oficial de Gitea&lt;/a&gt;, analicé la &lt;em&gt;chart&lt;/em&gt; oficial de Gitea. Después de revisarla, en esta entrada explico cómo instalar Gitea a partir de la &lt;em&gt;chart&lt;/em&gt; oficial usando Helm.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Revisión de la Helm chart oficial de Gitea</title>
      <link>https://onthedock.github.io/post/210529-revision-de-la-chart-de-gitea/</link>
      <pubDate>Sat, 29 May 2021 20:34:40 +0200</pubDate>
      <guid>https://onthedock.github.io/post/210529-revision-de-la-chart-de-gitea/</guid>
      <description>&lt;p&gt;Iba a escribir sobre las &lt;em&gt;lecciones aprendidas&lt;/em&gt; al intentar crear una &lt;em&gt;Helm Chart&lt;/em&gt; desde cero en una entrada cuando descubrí que &lt;a href=&#34;https://onthedock.github.io/post/210522-bug-no-se-muestran-imagenes-en-el-blog/&#34;&gt;las imágenes habían dejado de mostrarse en el blog&lt;/a&gt;. Eso me hizo reconducir mis esfuerzos en corregir el problema y dió al traste con la entrada que tenía a medias&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;Desde entonces he enfocado los esfuerzos en entender cómo funciona la &lt;em&gt;chart&lt;/em&gt; oficial para Gitea y en instalarla para ver qué posibilidades de personalización ofrece.&lt;/p&gt;&#xA;&lt;p&gt;En esta entrada me enfoco en la primera parte: analizar las elecciones realizadas por el equipo de Gitea a la hora de crear la &lt;em&gt;chart&lt;/em&gt; oficial.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Velero - Backup Y Disaster Recovery Para Kubernetes (III) Recuperación desde una copia de seguridad</title>
      <link>https://onthedock.github.io/post/210407-velero-backup-y-disaster-recovery-para-kubernetes-iii/</link>
      <pubDate>Wed, 07 Apr 2021 19:54:57 +0200</pubDate>
      <guid>https://onthedock.github.io/post/210407-velero-backup-y-disaster-recovery-para-kubernetes-iii/</guid>
      <description>&lt;p&gt;Velero realiza copias de seguridad (puntuales o recurrentes) para recuperarnos con rapidez de un desastre en el clúster de Kubernetes.&lt;/p&gt;&#xA;&lt;p&gt;En la entrada anterior &lt;a href=&#34;https://onthedock.github.io/post/210405-velero-backup-y-disaster-recovery-para-kubernetes-ii/&#34;&gt;Velero - Backup y Disaster Recovery para Kubernetes (II) Crear copia de seguridad&lt;/a&gt; desplegamos una aplicación de prueba en el clúster (dos réplicas de Nginx en el &lt;em&gt;Namespace&lt;/em&gt; &lt;code&gt;nginx-example&lt;/code&gt;). Simulamos la pérdida &amp;ldquo;accidental&amp;rdquo; del &lt;em&gt;Namespace&lt;/em&gt; &lt;code&gt;nginx-example&lt;/code&gt; para ver cómo recuperarnos usando la copia creada por Velero.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Velero - Backup y Disaster Recovery para Kubernetes (II) Crear copia de seguridad</title>
      <link>https://onthedock.github.io/post/210405-velero-backup-y-disaster-recovery-para-kubernetes-ii/</link>
      <pubDate>Mon, 05 Apr 2021 13:33:38 +0200</pubDate>
      <guid>https://onthedock.github.io/post/210405-velero-backup-y-disaster-recovery-para-kubernetes-ii/</guid>
      <description>&lt;p&gt;En la entrada anterior &lt;a href=&#34;https://onthedock.github.io/post/210405-velero-backup-y-disaster-recovery-para-kubernetes-ii/&#34;&gt;Velero - Backup y Disaster Recovery para Kubernetes (I) Instalación&lt;/a&gt; vimos cómo instalar Velero en Kubernetes. Usamos el comando &lt;code&gt;velero install&lt;/code&gt; lanzado desde un &lt;em&gt;Job&lt;/em&gt;  en la línea de la &lt;em&gt;filosofía&lt;/em&gt; GitOps.&lt;/p&gt;&#xA;&lt;p&gt;En esta entrada veremos cómo crear copias de seguridad puntuales y recurrentes usando la herramienta de línea de comandos &lt;strong&gt;velero&lt;/strong&gt; (lanzada desde un &lt;em&gt;Job&lt;/em&gt;)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Velero - Backup y Disaster Recovery para Kubernetes (I) Instalación</title>
      <link>https://onthedock.github.io/post/210405-velero-backup-y-disaster-recovery-para-kubernetes-i/</link>
      <pubDate>Mon, 05 Apr 2021 11:35:38 +0200</pubDate>
      <guid>https://onthedock.github.io/post/210405-velero-backup-y-disaster-recovery-para-kubernetes-i/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;htps:/velero.io&#34;&gt;Velero&lt;/a&gt; es una herramienta de código abierto que permite realizar copias de seguridad, restaurarlas y migrar recursos de Kubernetes entre clústers (lo que también permite recuperar un clúster en caso de desastre).&lt;/p&gt;&#xA;&lt;p&gt;Velero soporta diferentes proveedores en los que almacenar las copias de seguridad que realiza. La lista completa y actualizada se encuentra en &lt;a href=&#34;https://velero.io/docs/v1.5/supported-providers/&#34;&gt;Providers&lt;/a&gt;. En mi caso, voy a utilizar MinIO, que es compatible con AWS S3 y que tengo desplegado localmente en Kubernetes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Configura MinIO en Kubernetes usando Jobs y el cliente de MinIO &#39;mc&#39;</title>
      <link>https://onthedock.github.io/post/210403-configura-minio-en-k8s-usando-jobs-y-el-cliente-mc/</link>
      <pubDate>Sat, 03 Apr 2021 09:58:50 +0200</pubDate>
      <guid>https://onthedock.github.io/post/210403-configura-minio-en-k8s-usando-jobs-y-el-cliente-mc/</guid>
      <description>&lt;p&gt;En la entrada anterior, &lt;a href=&#34;https://onthedock.github.io/post/210403-minio-en-kubernetes/&#34;&gt;MinIO en Kubernetes&lt;/a&gt;, explicaba cómo desplegar MinIO en Kubernetes (de forma manual).&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://docs.min.io/docs/minio-client-quickstart-guide.html&#34;&gt;&lt;code&gt;mc&lt;/code&gt;&lt;/a&gt; se puede usar como herramienta de línea de comandos o como contenedor &lt;a href=&#34;https://hub.docker.com/r/minio/mc&#34;&gt;&lt;code&gt;minio/mc&lt;/code&gt;&lt;/a&gt;; sin embargo, en esta entrada veremos cómo usarlo en &lt;em&gt;Jobs&lt;/em&gt; de Kubernetes para configurar el &lt;em&gt;alias&lt;/em&gt; de un servidor de MinIO y crear un &lt;em&gt;bucket&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Minio en Kubernetes</title>
      <link>https://onthedock.github.io/post/210403-minio-en-kubernetes/</link>
      <pubDate>Sat, 03 Apr 2021 09:02:58 +0200</pubDate>
      <guid>https://onthedock.github.io/post/210403-minio-en-kubernetes/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://min.io/&#34;&gt;MinIO&lt;/a&gt; proporciona almacenamiento de objetos compatible con AWS S3. Desde la última vez que revisé este producto, MinIO ha crecido hasta convertirse en una solución de calidad empresarial.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;También han cambiado el logo, y por lo que veo, el &amp;ldquo;flamenco&amp;rdquo; o lo que sea el animal del logo ha quedado relegado al &lt;code&gt;favicon&lt;/code&gt; y el &lt;em&gt;footer&lt;/em&gt; de la página web&amp;hellip;Yo mantendré por ahora el logo &amp;ldquo;antiguo&amp;rdquo; (que curiosamente, tiene al pájaro mirando hacia la izquierda &lt;code&gt;¯\_(ツ)_/¯&lt;/code&gt;)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;En este artículo, voy a desplegarlo sobre Kubernetes de forma manual con un sólo servidor. Usaremos el almacenamiento en MinIO como destino de las copias de seguridad del clúster de Kubernetes usando Velero (en un próximo artículo).&lt;/p&gt;</description>
    </item>
    <item>
      <title>k3d: k3s en Docker (o la manera más rápida de montar clústers de Kubernetes para desarrollo)</title>
      <link>https://onthedock.github.io/post/210321-k3d-k3s-en-docker/</link>
      <pubDate>Sun, 21 Mar 2021 18:21:07 +0100</pubDate>
      <guid>https://onthedock.github.io/post/210321-k3d-k3s-en-docker/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://kind.sigs.k8s.io/&#34;&gt;kind&lt;/a&gt; es una herramienta para ejecutar clústers de Kubernetes en los que cada &amp;ldquo;nodo&amp;rdquo; del clúster se ejecuta en un contenedor. &lt;a href=&#34;https://k3d.io/&#34;&gt;k3d&lt;/a&gt; toma esta misma idea pero en vez de un clúster de Kubernetes &lt;em&gt;vanilla&lt;/em&gt;, despliega un clúster de &lt;strong&gt;&lt;a href=&#34;https://k3s.io/&#34;&gt;k3s&lt;/a&gt;&lt;/strong&gt; usando contenedores como &amp;ldquo;nodos&amp;rdquo;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Eliminar namespace encallado en Terminating</title>
      <link>https://onthedock.github.io/post/210305-eliminar-ns-terminating/</link>
      <pubDate>Fri, 05 Mar 2021 17:53:31 +0100</pubDate>
      <guid>https://onthedock.github.io/post/210305-eliminar-ns-terminating/</guid>
      <description>&lt;p&gt;Al hacer limpieza de uno de los clústers de desarrollo, he borrado dos &lt;em&gt;namespaces&lt;/em&gt; y los dos se han quedado en estado &lt;em&gt;Terminating&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Analiza los objetos de Kubernetes con Kubelinter y Cronjobs</title>
      <link>https://onthedock.github.io/post/210221-analiza-los-objetos-de-kubernetes-periodicamente-con-kubelinter-y-un-cronjob/</link>
      <pubDate>Sun, 21 Feb 2021 12:08:02 +0100</pubDate>
      <guid>https://onthedock.github.io/post/210221-analiza-los-objetos-de-kubernetes-periodicamente-con-kubelinter-y-un-cronjob/</guid>
      <description>&lt;p&gt;Idealmente, el análisis de los ficheros de definición de objetos (YAML) en Kubernetes debería realizarse &lt;strong&gt;antes&lt;/strong&gt; de crear los objetos en el clúster. Para ello, uno de los &lt;em&gt;stages&lt;/em&gt; del proceso de CI/CD debería incorporar KubeLinter (por ejemplo).&lt;/p&gt;&#xA;&lt;p&gt;De forma paralela, también deberíamos tener un proceso periódico que revise los ficheros de definición de los objetos que tenemos almacenados en el repositorio para identificar, por ejemplo, el uso de versiones de la API desaconsejadas (&lt;em&gt;deprecated&lt;/em&gt;) en proceso de eliminación de la API.&lt;/p&gt;&#xA;&lt;p&gt;En este artículo vemos cómo configurar un Cronjob que ejecute KubeLinter para obtener los ficheros de un repositorio remoto y analizarlos.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Seguridad en Kubernetes: runAsUser y readOnlyRootFilesystem</title>
      <link>https://onthedock.github.io/post/210212-runasuser-y-readonlyrootfilesystem/</link>
      <pubDate>Fri, 12 Feb 2021 23:02:05 +0100</pubDate>
      <guid>https://onthedock.github.io/post/210212-runasuser-y-readonlyrootfilesystem/</guid>
      <description>&lt;p&gt;En la entrada anterior &lt;a href=&#34;https://onthedock.github.io/post/210212-kubelinter/&#34;&gt;KubeLinter: identifica malas configuraciones en los objetos de Kubernetes&lt;/a&gt;, KubeLinter identificaba dos errores que se solucionan usando las opciones: &lt;code&gt;runAsUser&lt;/code&gt; y &lt;code&gt;readOnlyRootFilesystem&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;En esta entrada comparo los efectos de aplicar una u otra, así como qué pasa cuando se aplican las dos al mismo tiempo.&lt;/p&gt;</description>
    </item>
    <item>
      <title>KubeLinter: identifica malas configuraciones en los objetos de Kubernetes</title>
      <link>https://onthedock.github.io/post/210212-kubelinter/</link>
      <pubDate>Fri, 12 Feb 2021 21:36:01 +0100</pubDate>
      <guid>https://onthedock.github.io/post/210212-kubelinter/</guid>
      <description>&lt;p&gt;KubeLinter es un &lt;a href=&#34;https://es.wikipedia.org/wiki/Lint&#34;&gt;&lt;em&gt;linter&lt;/em&gt;&lt;/a&gt; para los objetos de Kubernetes; es decir, KubeLinter comprueba configuraciones &lt;em&gt;sospechosas&lt;/em&gt; en los ficheros de definición de los objetos de Kubernetes.&lt;/p&gt;&#xA;&lt;p&gt;En la documentación oficial tienes una lista de las validaciones que realiza y cuáles vienen habilitadas por defecto: &lt;a href=&#34;https://docs.kubelinter.io/#/generated/checks&#34;&gt;KubeLinter checks&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;KubeLinter es una herramienta &lt;em&gt;opensource&lt;/em&gt; desarrollada por StackRox, una empresa orientada a la seguridad que recientemente ha sido adquirida por Red Hat, precisamente, para mejorar la seguridad de OpenShift.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cómo saber si un recurso de Kubernetes está restringido al namespace o es global</title>
      <link>https://onthedock.github.io/post/201224-como-saber-si-un-recursos-esta-restringido-a-un-namespace/</link>
      <pubDate>Thu, 24 Dec 2020 11:39:07 +0100</pubDate>
      <guid>https://onthedock.github.io/post/201224-como-saber-si-un-recursos-esta-restringido-a-un-namespace/</guid>
      <description>&lt;p&gt;La mayoría de los recursos de Kubernetes como los &lt;em&gt;pods&lt;/em&gt;, los &lt;em&gt;services&lt;/em&gt;, los &lt;em&gt;replication controllers&lt;/em&gt;, etc están limitados al &lt;em&gt;namespace&lt;/em&gt; en el que se despliegan. Así, dos recursos pueden tener el mismo nombre, etc si se encuentran en &lt;em&gt;namespaces&lt;/em&gt; diferentes, ya que el &lt;em&gt;namespace&lt;/em&gt; define el &lt;em&gt;alcance&lt;/em&gt; de visibilidad para el recursos. El &lt;em&gt;namespace&lt;/em&gt; es el límite del &lt;em&gt;scope&lt;/em&gt; del recurso en Kubernetes.&lt;/p&gt;&#xA;&lt;p&gt;Sin embargo, no todos los recursos en Kubernetes se encuentran &amp;ldquo;limitados&amp;rdquo; por el &lt;em&gt;namespace&lt;/em&gt;; por ejemplo, el propio recurso &lt;code&gt;namespace&lt;/code&gt; no está en un &lt;em&gt;namespace&lt;/em&gt;, ni los &lt;code&gt;persistentVolumes&lt;/code&gt; tampoco&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;¿Cómo podemos obtener una lista de los recursos con alcance restringido al &lt;em&gt;namespace&lt;/em&gt; en el que se encuentran?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Desplegar Gitea en Kubernetes</title>
      <link>https://onthedock.github.io/post/201212-desplegar-gitea-en-kubernetes/</link>
      <pubDate>Sat, 12 Dec 2020 12:13:18 +0100</pubDate>
      <guid>https://onthedock.github.io/post/201212-desplegar-gitea-en-kubernetes/</guid>
      <description>&lt;p&gt;Ya he hablado varias veces de &lt;a href=&#34;https://onthedock.github.io/tags/gitea&#34;&gt;Gitea en este sitio&lt;/a&gt;, así que no me repetiré (mucho)&#xA;; Gitea es una solución ligera de alojamiento de repositorios Git (a lo GitHub).&lt;/p&gt;&#xA;&lt;p&gt;En esta entrada se indica el proceso que he seguido para la creación de los diferentes objetos necesarios para desplegar Gitea (usando SQLite como base de datos) en Kubernetes.&lt;/p&gt;&#xA;&lt;p&gt;Puedes seguir los pasos de la &lt;a href=&#34;https://docs.gitea.io/en-us/install-on-kubernetes/&#34;&gt;documentación oficial para desplegar Gitea&lt;/a&gt; sobre Kubernetes usando Helm.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Actualizar el certificado de Traefik en K3s</title>
      <link>https://onthedock.github.io/post/201208-actualizar-el-certificado-de-traefik-en-k3s/</link>
      <pubDate>Tue, 08 Dec 2020 09:09:23 +0100</pubDate>
      <guid>https://onthedock.github.io/post/201208-actualizar-el-certificado-de-traefik-en-k3s/</guid>
      <description>&lt;p&gt;K3OS (y K3s) despliega Traefik como Ingress. Pero el problema es que el certificado autofirmado configurado por defecto caducó en 2017.&lt;/p&gt;&#xA;&lt;p&gt;Probablemente se trata de una &lt;em&gt;feature&lt;/em&gt; y no de un &lt;em&gt;bug&lt;/em&gt;, para &amp;ldquo;animar&amp;rdquo; a cambiar el certificado desplegado por defecto por uno válido; en este artículo explico cómo hacerlo.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Configurar hostname en K3OS</title>
      <link>https://onthedock.github.io/post/200923-configurar-hostname-en-k3os/</link>
      <pubDate>Wed, 23 Sep 2020 21:53:31 +0200</pubDate>
      <guid>https://onthedock.github.io/post/200923-configurar-hostname-en-k3os/</guid>
      <description>&lt;p&gt;Una de las cosas que no me resultó evidente al empezar a usar &lt;a href=&#34;https://k3os.io&#34;&gt;K3OS&lt;/a&gt; es que el sistema de ficheros tiene algunas &lt;em&gt;particularidades&lt;/em&gt; con las que es &lt;strong&gt;absolutamente&lt;/strong&gt; necesario estar familiarizado; por ejemplo, que toda la carpeta &lt;code&gt;/etc&lt;/code&gt; es &lt;strong&gt;EFÍMERA&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Generando un machine-id único</title>
      <link>https://onthedock.github.io/post/180810-generando-un-machine-id-unico/</link>
      <pubDate>Fri, 10 Aug 2018 18:10:27 +0200</pubDate>
      <guid>https://onthedock.github.io/post/180810-generando-un-machine-id-unico/</guid>
      <description>&lt;p&gt;En la entrada &lt;a href=&#34;https://onthedock.github.io/post/180610-pods-en-estado-creatingcontainer-en-k8s/&#34;&gt;Pods en estado creatingContainer en K8s&lt;/a&gt; describía el problema surgido al crear un clúster de Kubernetes usando Vagrant. Al partir de la misma imagen, todas las máquinas del clúster tienen el mismo &lt;code&gt;machine-id&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;El &lt;code&gt;machine-id&lt;/code&gt; debe ser único, como se describe en los &lt;a href=&#34;https://kubernetes.io/docs/setup/independent/install-kubeadm/#verify-the-mac-address-and-product-uuid-are-unique-for-every-node&#34;&gt;requerimientos de Kubernetes&lt;/a&gt;; si no lo es, se producen problemas como el descrito.&lt;/p&gt;&#xA;&lt;p&gt;En esta entrada analizo con más detalle cómo se crea el &lt;em&gt;machine-id&lt;/em&gt; y cómo generar uno nuevo.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pods encallados en estado CreatingContainer en Kubernetes con nodos creados usando Vagrant</title>
      <link>https://onthedock.github.io/post/180610-pods-en-estado-creatingcontainer-en-k8s/</link>
      <pubDate>Sun, 10 Jun 2018 20:54:27 +0200</pubDate>
      <guid>https://onthedock.github.io/post/180610-pods-en-estado-creatingcontainer-en-k8s/</guid>
      <description>&lt;p&gt;Una de las maneras más sencillas de crear un entorno de desarrollo para Kubernetes es usando Vagrant y Ansible.&lt;/p&gt;&#xA;&lt;p&gt;En el &lt;code&gt;Vagrantfile&lt;/code&gt; definimos un conjunto de tres máquinas, llamadas &lt;code&gt;node1&lt;/code&gt;, &lt;code&gt;node2&lt;/code&gt; y &lt;code&gt;node3&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Una vez las máquinas están levantadas, desde el servidor de Ansible uso &lt;code&gt;ssh-copy-id&lt;/code&gt; para habilitar el &lt;em&gt;login&lt;/em&gt; sin password de Ansible en los nodos del clúster.&lt;/p&gt;&#xA;&lt;p&gt;A partir de aquí, tanto la instalación de los prerequisitos como la inicialización del clúster funcionan sin problemas; sin embargo, al intentar desplegar una aplicación, los &lt;em&gt;pods&lt;/em&gt; se quedan en el estado &lt;em&gt;CreatingContainer&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Single node cluster en la RPi3</title>
      <link>https://onthedock.github.io/post/171111-snc-en-rpi3/</link>
      <pubDate>Sat, 11 Nov 2017 12:08:36 +0100</pubDate>
      <guid>https://onthedock.github.io/post/171111-snc-en-rpi3/</guid>
      <description>&lt;p&gt;En la entrada anterior &lt;a href=&#34;https://onthedock.github.io/post/171104-apiserver-detenido/&#34;&gt;API server detenido: The connection to the server was refused&lt;/a&gt; encontré problemas con la tarjeta microSD que sirve de almacenamiento para el nodo master del clúster de Kubernetes.&lt;/p&gt;&#xA;&lt;p&gt;La solución al problema pasaba por realizar un análisis de la tarjeta para repararla. Sin embargo, al intentarlo, no ha habido manera de formatear y reinstalar HypriotOS sobre la tarjeta.&lt;/p&gt;&#xA;&lt;p&gt;El fallo de la tarjeta de memoria ha sido la gota final que me ha hecho abandonar el clúster multinodo en las Raspberry Pi (de momento). Así que he decidido instalar un clúster de un solo nodo en una de las Raspberri Pi 3.&lt;/p&gt;&#xA;&lt;p&gt;En este artículo sigo las instrucciones oficiales para construir un clúster de Kubernetes usando &lt;em&gt;kubeadm&lt;/em&gt;: &lt;a href=&#34;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&#34;&gt;Using kubeadm to Create a Cluster&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>API server detenido: The connection to the server was refused</title>
      <link>https://onthedock.github.io/post/171104-apiserver-detenido/</link>
      <pubDate>Sat, 04 Nov 2017 21:58:52 +0100</pubDate>
      <guid>https://onthedock.github.io/post/171104-apiserver-detenido/</guid>
      <description>&lt;p&gt;Hoy, al intentar lanzar un comando con &lt;code&gt;kubectl&lt;/code&gt;, he obtenido el típico mensaje indicando que no se puede conectar con el servidor. El problema está en el servidor de API, que es el que actua como intermediario entre el usuario y Kubernetes. Últimamente he encontrado el mismo error y lo he solucionado reiniciando el nodo &lt;em&gt;master&lt;/em&gt; del clúster. Pero hoy he investigado un poco&amp;hellip; Y lo que he encontrado no me ha gustado demasiado.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Usando un contenedor sidecar para el almacenamiento</title>
      <link>https://onthedock.github.io/post/170819-usando-un-contenedor-sidecar/</link>
      <pubDate>Sat, 19 Aug 2017 09:51:23 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170819-usando-un-contenedor-sidecar/</guid>
      <description>&lt;p&gt;Como indicaba en la entrada la anterior entrada &lt;a href=&#34;https://onthedock.github.io/post/170817-almacenamiento-en-k8s-problema-abierto/&#34;&gt;Almacenamiento en Kubernetes: problema abierto&lt;/a&gt;, el problema de proporcionar almacenamiento persistente para las aplicaciones desplegadas en Kubernetes sigue sin tener una solución general.&lt;/p&gt;&#xA;&lt;p&gt;En este artículo comento una solución particular al problema del almacenamiento basada en el uso de un contenedor &lt;em&gt;sidecar&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Play With Kubernetes</title>
      <link>https://onthedock.github.io/post/170818-play-with-k8s/</link>
      <pubDate>Fri, 18 Aug 2017 20:25:31 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170818-play-with-k8s/</guid>
      <description>&lt;p&gt;Hace unas semanas descubrí el sitio &lt;a href=&#34;http://play-with-k8s.com&#34;&gt;PWK&lt;/a&gt;, &lt;strong&gt;Play with Kubernetes&lt;/strong&gt;. Su creador, &lt;a href=&#34;https://medium.com/@marcosnils&#34;&gt;Marcos Nils&lt;/a&gt; explica en &lt;a href=&#34;https://medium.com/@marcosnils/introducing-pwk-play-with-k8s-159fcfeb787b&#34;&gt;Introducing PWK (Play with K8s)&lt;/a&gt; que tenía ganas de extender la plataforma PWD (Play with Docker) a Kubernetes.&lt;/p&gt;&#xA;&lt;p&gt;El sitio PWK permite montar clústers de Kubernetes y lanzar servicios replicados de manera rápida y sencilla. Se trata de un entorno donde realizar pruebas y &lt;em&gt;jugar&lt;/em&gt; durante cuatro horas con varias instancias de Docker sobre las que podemos usar &lt;code&gt;kubeadm&lt;/code&gt; para instalar y configurar Kubernetes, creando un clúster en menos de un minuto.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Troubleshooting: Creación de pods del tutorial &#39;StatefulSet Basics&#39;</title>
      <link>https://onthedock.github.io/post/170818-troubleshooting-creacion-de-pods-del-tutorial-statefulset-basics/</link>
      <pubDate>Fri, 18 Aug 2017 17:45:03 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170818-troubleshooting-creacion-de-pods-del-tutorial-statefulset-basics/</guid>
      <description>&lt;p&gt;Esta entrada es un registro de las diferentes acciones que realicé para conseguir que los &lt;em&gt;pods&lt;/em&gt; asociados al &lt;em&gt;StatefulSet&lt;/em&gt; del tutorial &lt;a href=&#34;https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/&#34;&gt;StatefulSet Basics&lt;/a&gt; se crearan correctamente.&lt;/p&gt;&#xA;&lt;p&gt;Lo publico como lo que es, un &lt;em&gt;log&lt;/em&gt; de todos los pasos que fui dando, en modo &lt;em&gt;ensayo y error&lt;/em&gt;, hasta que conseguí que los &lt;em&gt;pods&lt;/em&gt; se crearan con éxito. Mi intención al publicarlo no es tanto que sirva como referencia sino como archivo. Y si alguien se encuentra con un problema similar, que pueda consultar los pasos que he dado durante el &lt;em&gt;troubleshooting&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Como indicaba en el artículo anterior, quiero publicar un tutorial paso a paso con el proceso correcto para provisionar los &lt;em&gt;PersistentVolumes&lt;/em&gt; necesarios para el tutorial &lt;a href=&#34;https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/&#34;&gt;StatefulSet Basics&lt;/a&gt; del sitio de Kubernetes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Almacenamiento en Kubernetes: problema abierto</title>
      <link>https://onthedock.github.io/post/170817-almacenamiento-en-k8s-problema-abierto/</link>
      <pubDate>Thu, 17 Aug 2017 17:11:05 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170817-almacenamiento-en-k8s-problema-abierto/</guid>
      <description>&lt;p&gt;Leía el otro día que desde el principio la tendencia hacia los microservicios estaba pensada para las aplicaciones &lt;em&gt;stateless&lt;/em&gt;, es decir, sin &amp;ldquo;memoria&amp;rdquo; del estado, donde cada interacción con la aplicación se considera independiente del resto. El ejemplo clásico de aplicación &lt;em&gt;stateless&lt;/em&gt; es un servidor web. Así que no es de extrañar que la aplicación que siempre aparece en todo tutorial que se precie de Docker/Kubernetes  es Nginx.&lt;/p&gt;&#xA;&lt;p&gt;En el mundo real, sin embargo, la mayoría de aplicaciones requieren algún tipo de persistencia, incluso las webs más sencillas (así surgieron las &lt;em&gt;cookies&lt;/em&gt;). Pero por el momento, Kubernetes y el almacenamiento son dos conceptos que no combinan demasiado bien, aunque funcionan perfectamente por separado.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Asignar recursos de CPU y RAM a un contenedor</title>
      <link>https://onthedock.github.io/post/170729-asignar-recursos-de-cpu-y-ram-a-un-contenedor/</link>
      <pubDate>Sat, 29 Jul 2017 21:12:35 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170729-asignar-recursos-de-cpu-y-ram-a-un-contenedor/</guid>
      <description>&lt;p&gt;Cuando se crea un &lt;em&gt;pod&lt;/em&gt; se pueden reservar recursos de CPU y RAM para los contenedores que corren en el &lt;em&gt;pod&lt;/em&gt;. Para reservar recursos, usa el campo &lt;code&gt;resources: requests&lt;/code&gt; en el fichero de configuración. Para establecer límites, usa el campo &lt;code&gt;resources: limits&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Espacios de nombres en Kubernetes</title>
      <link>https://onthedock.github.io/post/170723-espacios-de-nombres-en-k8s/</link>
      <pubDate>Sun, 23 Jul 2017 20:04:45 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170723-espacios-de-nombres-en-k8s/</guid>
      <description>&lt;p&gt;Los &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/&#34;&gt;&lt;em&gt;namespaces&lt;/em&gt; (espacios de nombres)&lt;/a&gt; en Kubernetes permiten establecer un nivel adicional de separación entre los contenedores que comparten los recursos de un clúster.&lt;/p&gt;&#xA;&lt;p&gt;Esto es especialmente útil cuando diferentes grupos de DevOps usan el mismo clúster y existe el riesgo potencial de colisión de nombres de los &lt;em&gt;pods&lt;/em&gt;, etc usados por los diferentes equipos.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mi primera aplicación en Kubernetes</title>
      <link>https://onthedock.github.io/post/170716-mi-primera-app-en-kubernetes/</link>
      <pubDate>Sun, 16 Jul 2017 19:38:17 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170716-mi-primera-app-en-kubernetes/</guid>
      <description>&lt;p&gt;Después de &lt;a href=&#34;https://onthedock.github.io/post/170702-crear-un-cluster-de-un-solo-nodo/&#34;&gt;crear un cluster de un solo nodo&lt;/a&gt;, en esta entrada explico los pasos para publicar una aplicación en el clúster.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Crear un cluster de un solo nodo</title>
      <link>https://onthedock.github.io/post/170702-crear-un-cluster-de-un-solo-nodo/</link>
      <pubDate>Sun, 02 Jul 2017 23:14:22 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170702-crear-un-cluster-de-un-solo-nodo/</guid>
      <description>&lt;p&gt;Para tener un clúster de desarrollo con la versatilidad de poder hacer y deshacer cambios (usando los &lt;em&gt;snapshots&lt;/em&gt; de una máquina virtual), lo más sencillo es disponer de un clúster de Kubernetes de un solo nodo.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Revisión de conceptos</title>
      <link>https://onthedock.github.io/post/170528-revision-de-conceptos/</link>
      <pubDate>Sun, 28 May 2017 07:59:31 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170528-revision-de-conceptos/</guid>
      <description>&lt;p&gt;Después de estabilizar el clúster, el siguiente paso es poner en marcha aplicaciones. Pero ¿qué es exactamente lo que hay que desplegar?: ¿&lt;em&gt;pods&lt;/em&gt;?, ¿&lt;em&gt;replication controllers&lt;/em&gt;?, ¿&lt;em&gt;deployments&lt;/em&gt;?&lt;/p&gt;&#xA;&lt;p&gt;Muchos artículos empiezan creando el fichero YAML para un &lt;em&gt;pod&lt;/em&gt;, después construyen el &lt;em&gt;replication controller&lt;/em&gt;, etc&amp;hellip; Sin embargo, revisando la documentación oficial, crear &lt;em&gt;pods&lt;/em&gt; directamente en Kubernetes no tiene mucho sentido.&lt;/p&gt;&#xA;&lt;p&gt;En este artículo intento determinar qué objetos son los que deben crearse en un clúster Kubernetes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>El nodo k3 sigue colgandose por culpa de Flannel</title>
      <link>https://onthedock.github.io/post/170517-el-nodo-k3-sigue-colgandose-por-culpa-de-flannel/</link>
      <pubDate>Wed, 17 May 2017 21:02:21 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170517-el-nodo-k3-sigue-colgandose-por-culpa-de-flannel/</guid>
      <description>&lt;p&gt;En la entrada &lt;a href=&#34;https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/&#34;&gt;Troubleshooting Kubernetes (II)&lt;/a&gt; encontré restos de la instalación de &lt;a href=&#34;https://github.com/coreos/flannel&#34;&gt;Flannel&lt;/a&gt; en la Raspberry Pi. Eliminé los &lt;em&gt;pods&lt;/em&gt; que hacían referencia a Flannel y conseguí que el nodo &lt;strong&gt;k2&lt;/strong&gt; no se volviera a colgar.&lt;/p&gt;&#xA;&lt;p&gt;Sin embargo, el problema sigue dándose en el nodo &lt;strong&gt;k3&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Revisando el contenido de &lt;code&gt;/var/lib/kubernetes/pods/&lt;/code&gt; he visto que algunos &lt;em&gt;pods&lt;/em&gt; hacían referencia, todavía, a Flannel.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Troubleshooting Kubernetes (II)</title>
      <link>https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/</link>
      <pubDate>Sat, 06 May 2017 05:21:09 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/</guid>
      <description>&lt;p&gt;Sigo con el &lt;em&gt;troubleshooting&lt;/em&gt; del &lt;em&gt;cuelgue&lt;/em&gt; de los nodos sobre Raspberry Pi 3 del clúster.&lt;/p&gt;&#xA;&lt;p&gt;Ayer estuve &lt;em&gt;haciendo limpieza&lt;/em&gt; siguiendo &lt;em&gt;vagamente&lt;/em&gt; la recomendación de &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/43593#issuecomment-288899231&#34;&gt;esta respuesta&lt;/a&gt; en el hilo &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/43593#issuecomment-288899231&#34;&gt;Kubernetes memory consumption explosion&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Instala Weave Net en Kubernetes 1.6</title>
      <link>https://onthedock.github.io/post/170505-instala-weave-net-en-kubernetes-1.6/</link>
      <pubDate>Fri, 05 May 2017 22:14:36 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170505-instala-weave-net-en-kubernetes-1.6/</guid>
      <description>&lt;p&gt;Una de las cosas que más me sorprenden de Kubernetes es que es necesario instalar una &lt;em&gt;capa de red&lt;/em&gt; sobre el clúster.&lt;/p&gt;&#xA;&lt;p&gt;En el caso concreto del que he obtenido las &lt;em&gt;capturas de pantalla&lt;/em&gt;, el clúster corre sobre máquinas virtuales con Debian Jessie.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Troubleshooting Kubernetes (I)</title>
      <link>https://onthedock.github.io/post/170430-troubleshooting-kubernetes-i/</link>
      <pubDate>Sun, 30 Apr 2017 15:24:35 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170430-troubleshooting-kubernetes-i/</guid>
      <description>&lt;p&gt;Tras la alegría inicial pensando que la configuración de &lt;em&gt;rsyslog&lt;/em&gt; era la causante de los cuelgues de las dos RPi 3 (&lt;a href=&#34;https://onthedock.github.io/post/170430-k3-colgado-de-nuevo/&#34;&gt;El nodo k3 del clúster colgado de nuevo&lt;/a&gt;), pasadas unas horas los dos nodos &lt;strong&gt;k2&lt;/strong&gt; y &lt;strong&gt;k3&lt;/strong&gt; han dejado de responder de nuevo.&lt;/p&gt;&#xA;&lt;p&gt;Así que es el momento de atacar el problema de forma algo más sistemática. Para ello seguiré las instrucciones que proporcina la página de Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/&#34;&gt;Troubleshooting Clusters&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Errores sobre Orphaned pods en syslog</title>
      <link>https://onthedock.github.io/post/170430-errores-sobre-orphaned-pods-en-syslog/</link>
      <pubDate>Sun, 30 Apr 2017 12:55:44 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170430-errores-sobre-orphaned-pods-en-syslog/</guid>
      <description>&lt;p&gt;Los nodos &lt;strong&gt;k2&lt;/strong&gt; y &lt;strong&gt;k3&lt;/strong&gt; del clúster dejan de responder pasadas unas horas. La única manera de solucionarlo es reiniciar los nodos. Siguiendo con la revisión de logs, he encontrado que se genera una gran cantidad de entradas en &lt;em&gt;syslog&lt;/em&gt; en referencia a &lt;em&gt;orphaned pods&lt;/em&gt;. Además, el número de estos errores no para de crecer &lt;strong&gt;rápidamente&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>El nodo k3 del clúster colgado de nuevo</title>
      <link>https://onthedock.github.io/post/170430-k3-colgado-de-nuevo/</link>
      <pubDate>Sun, 30 Apr 2017 11:39:20 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170430-k3-colgado-de-nuevo/</guid>
      <description>&lt;p&gt;En la entrada anterior &lt;a href=&#34;https://onthedock.github.io/post/170430-multiples-mensajes-action-17-suspended/&#34;&gt;Múltiples mensajes &amp;lsquo;action 17 suspended&amp;rsquo; en los logs&lt;/a&gt; comentaba que estaba a la espera de obtener resultados; después de apenas unas horas, ya los tengo: &lt;strong&gt;k3&lt;/strong&gt; se ha vuelto a &lt;em&gt;colgar&lt;/em&gt; mientras que &lt;strong&gt;k2&lt;/strong&gt; no.&lt;/p&gt;&#xA;&lt;p&gt;Este resultado parece demostrar que la mala configuración de &lt;em&gt;rsyslog&lt;/em&gt; es la causante de los &lt;em&gt;cuelgues&lt;/em&gt; de las RPi 3 en el clúster de Kubernetes.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Actualización&lt;/strong&gt;: El nodo &lt;strong&gt;k2&lt;/strong&gt; sobre RPi3 sigue colgándose :(&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Actualización II&lt;/strong&gt;: &lt;a href=&#34;https://onthedock.github.io/post/170506-troubleshooting-kubernetes-ii/&#34;&gt;Parece solucionado&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Solución al error de instalación de Kubernetes en Debian Jessie (Missing cgroups: memory)</title>
      <link>https://onthedock.github.io/post/170422-solucion-al-error-missing-cgroups-memory-en-debian-jessie/</link>
      <pubDate>Sat, 22 Apr 2017 07:57:14 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170422-solucion-al-error-missing-cgroups-memory-en-debian-jessie/</guid>
      <description>&lt;p&gt;Al lanzar la inicialización del clúster con &lt;code&gt;kubeadm init&lt;/code&gt; en Debian Jessie, las comprobaciones inciales indican que no se encuentran los &lt;em&gt;cgroups&lt;/em&gt; para la memoria (échale un vistazo al artículo &lt;a href=&#34;https://onthedock.github.io/post/170417-instalacion-de-kubernetes-falla-missing-cgroups-memory/&#34;&gt;La instalación de Kubernetes falla en Debian Jessie (missing cgroups: memory)&lt;/a&gt;). Los &lt;em&gt;cgroups&lt;/em&gt; son una de las piezas fundamentales en las que se basa Docker para &lt;em&gt;aislar&lt;/em&gt; los procesos de los contenedores, por lo que la inicialización del clúster de Kubernetes se detiene.&lt;/p&gt;&#xA;&lt;p&gt;La solución es tan sencilla como habilitar los &lt;em&gt;cgroups&lt;/em&gt; durante el arranque.&lt;/p&gt;</description>
    </item>
    <item>
      <title>La instalación de Kubernetes falla en Debian Jessie (Missing cgroups: memory)</title>
      <link>https://onthedock.github.io/post/170417-instalacion-de-kubernetes-falla-missing-cgroups-memory/</link>
      <pubDate>Mon, 17 Apr 2017 19:38:11 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170417-instalacion-de-kubernetes-falla-missing-cgroups-memory/</guid>
      <description>&lt;p&gt;La instalación de Kubernetes se realiza de forma casi automática gracias al &lt;em&gt;script&lt;/em&gt; &lt;code&gt;kubeadm&lt;/code&gt;. Sólo hay que seguir las instrucciones de &lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/kubeadm/&#34;&gt;Installing Kubernetes on Linux with kubeadm&lt;/a&gt; y la salida por pantalla del propio &lt;em&gt;script&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cómo agregar un nodo a un cluster Kubernetes</title>
      <link>https://onthedock.github.io/post/170417-como-agregar-un-nodo-a-un-cluster-kubernetes/</link>
      <pubDate>Sat, 15 Apr 2017 16:27:30 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170417-como-agregar-un-nodo-a-un-cluster-kubernetes/</guid>
      <description>&lt;p&gt;Después de realizar la instalación del nodo &lt;em&gt;master&lt;/em&gt; del clúster Kubernetes, el siguiente paso es agregar nodos adicionales al clúster. Es en estos nodos donde se van a planificar los &lt;em&gt;pods&lt;/em&gt; que realizan las funciones &lt;em&gt;productivas&lt;/em&gt; del clúster (en el nodo &lt;em&gt;master&lt;/em&gt; sólo realiza tareas de gestión del clúster).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Error: The connection to the server localhost:8080 was refused</title>
      <link>https://onthedock.github.io/post/170414-error_the-connection-to-the-server-was-refused/</link>
      <pubDate>Fri, 14 Apr 2017 18:10:34 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170414-error_the-connection-to-the-server-was-refused/</guid>
      <description>&lt;p&gt;Después de &lt;a href=&#34;https://onthedock.github.io/post/170410-k8s-en-rpi-teaser/&#34;&gt;conseguir arrancar Kubernetes tras la instalación&lt;/a&gt;, al intentar ejecutar comandos vía &lt;code&gt;kubectl&lt;/code&gt; obtengo el mensaje de error &lt;code&gt;The connection to the server localhost:8080 was refused - did you specify the right host or port?&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;A continuación explico cómo solucionar el error y evitar que vuelva a mostrarse.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kubernetes en la Raspberry Pi (teaser)</title>
      <link>https://onthedock.github.io/post/170410-k8s-en-rpi-teaser/</link>
      <pubDate>Mon, 10 Apr 2017 22:45:28 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170410-k8s-en-rpi-teaser/</guid>
      <description>&lt;figure&gt;&lt;img src=&#34;https://onthedock.github.io/images/170410/itsalive.jpg&#34;&gt;&#xA;&lt;/figure&gt;</description>
    </item>
    <item>
      <title>Acciones previas a la instalación de Kubernetes en Raspberry Pi</title>
      <link>https://onthedock.github.io/post/170409-acciones-previas-instalacion-rpi/</link>
      <pubDate>Sun, 09 Apr 2017 21:34:16 +0200</pubDate>
      <guid>https://onthedock.github.io/post/170409-acciones-previas-instalacion-rpi/</guid>
      <description>&lt;p&gt;Uno de los objetivos motivadores de la existencia de este blog es instalar un clúster de Kubernetes sobre Raspberry Pi. Este artículo se centra en las tareas previas a la instalación en sí.&lt;/p&gt;&#xA;&lt;p&gt;Kubernetes requiere una instalación previa de Docker, una tarea simplificada gracias a HypriotOS, la &lt;em&gt;distro&lt;/em&gt; creada específicamente con este fin.&lt;/p&gt;&#xA;&lt;p&gt;El siguiente paso, la instalación de Kubernetes en la Raspberry será objeto de otra(s) entrada(s). Pero sin duda esta tarea sería mucho más complicada sin las contribuciones del joven finlandés &lt;a href=&#34;https://www.cncf.io/blog/2016/11/29/diversity-scholarship-series-programming-journey-becoming-kubernetes-maintainer/&#34;&gt;Lucas Käldström&lt;/a&gt; y su proyecto -ahora integrado la rama principal- &lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;Kubernetes on ARM&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
